{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/congningni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/congningni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "  \n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "  \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Flatten\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Convolution1D\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "import random\n",
    "import string\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "import sklearn.metrics as mtc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./Dataset/training_sep09.csv')\n",
    "df_test = pd.read_csv('./Dataset/testing_sep09.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4649a1d3164b44acf71eac792e6c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=53924.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63325171e9a248b29c0f424ab4ac4c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=13631.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_train['Content_list'] = df_train.Content.swifter.apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "df_train['Content_list'] = df_train.Content_list.str.split(' ')\n",
    "\n",
    "df_test['Content_list'] = df_test.Content.swifter.apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "df_test['Content_list'] = df_test.Content_list.str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Label', 'Content', 'Content_list'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content_all = df_train.append(df_test[['Label', 'Content', 'Content_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 50000\n",
    "EMBED_SIZE = 128\n",
    "RNN_CELL_SIZE = 128\n",
    "MAX_LEN = 200  \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(df_content_all.Content_list)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df_train.Content_list)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(df_test.Content_list)\n",
    "\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=MAX_LEN)\n",
    "y_train = pd.get_dummies(df_train.Label).values\n",
    "\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=MAX_LEN)\n",
    "y_test = pd.get_dummies(df_test.Label).values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(df_test.Content_list)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(df_test.Content_list)\n",
    "  \n",
    "#MAX_LEN = 200  \n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=MAX_LEN)\n",
    "y_test = pd.get_dummies(df_test.Label).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "          \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = tf.nn.tanh(\n",
    "            self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "          \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reset the model\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "\n",
    "sequence_input = Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "embedded_sequences = Embedding(MAX_FEATURES, EMBED_SIZE)(sequence_input)\n",
    "\n",
    "lstm = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences = True), name=\"bi_lstm_0\")(embedded_sequences)\n",
    "\n",
    "# Getting our LSTM outputs\n",
    "(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences=True, return_state=True), \n",
    "                                                                     name=\"bi_lstm_1\")(lstm)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "context_vector, attention_weights = Attention(10)(lstm, state_h)\n",
    "dropout1 = Dropout(0.2)(context_vector)\n",
    "dense1 = Dense(20, activation=\"relu\")(dropout1)\n",
    "dropout2 = Dropout(0.2)(dense1)\n",
    "output = Dense(3, activation=\"softmax\")(dropout2)\n",
    "\n",
    "#model = keras.Model(inputs=sequence_input, outputs=output)\n",
    "\n",
    "METRICS = [\n",
    "keras.metrics.TruePositives(name='tp'),\n",
    "keras.metrics.FalsePositives(name='fp'),\n",
    "keras.metrics.TrueNegatives(name='tn'),\n",
    "keras.metrics.FalseNegatives(name='fn'),\n",
    "keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "keras.metrics.Precision(name='precision'),\n",
    "keras.metrics.Recall(name='recall'),\n",
    "keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "###\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 128)     6400000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_0 (Bidirectional)       (None, 200, 256)     263168      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bi_lstm_1 (Bidirectional)       [(None, 200, 256), ( 394240      bi_lstm_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256)          0           bi_lstm_1[0][1]                  \n",
      "                                                                 bi_lstm_1[0][3]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         ((None, 256), (None, 5151        bi_lstm_1[0][0]                  \n",
      "                                                                 concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 20)           5140        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 3)            63          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,067,762\n",
      "Trainable params: 7,067,762\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "338/338 [==============================] - 612s 2s/step - loss: 0.8638 - tp: 11468.9292 - fp: 4758.2242 - tn: 38759.5457 - fn: 10289.9558 - accuracy: 0.7497 - precision: 0.6629 - recall: 0.4721 - auc: 0.7857 - val_loss: 0.6362 - val_tp: 7379.0000 - val_fp: 2219.0000 - val_tn: 19351.0000 - val_fn: 3406.0000 - val_accuracy: 0.8261 - val_precision: 0.7688 - val_recall: 0.6842 - val_auc: 0.8951\n",
      "Epoch 2/5\n",
      "338/338 [==============================] - 576s 2s/step - loss: 0.5652 - tp: 15896.3481 - fp: 3343.9912 - tn: 40173.7788 - fn: 5862.5369 - accuracy: 0.8552 - precision: 0.8222 - recall: 0.7213 - auc: 0.9161 - val_loss: 0.6002 - val_tp: 7631.0000 - val_fp: 1933.0000 - val_tn: 19637.0000 - val_fn: 3154.0000 - val_accuracy: 0.8428 - val_precision: 0.7979 - val_recall: 0.7076 - val_auc: 0.9055\n",
      "Epoch 3/5\n",
      "338/338 [==============================] - 532s 2s/step - loss: 0.4341 - tp: 17590.5841 - fp: 2752.7434 - tn: 40765.0265 - fn: 4168.3009 - accuracy: 0.8939 - precision: 0.8652 - recall: 0.8077 - auc: 0.9499 - val_loss: 0.5975 - val_tp: 7744.0000 - val_fp: 1923.0000 - val_tn: 19647.0000 - val_fn: 3041.0000 - val_accuracy: 0.8466 - val_precision: 0.8011 - val_recall: 0.7180 - val_auc: 0.9079\n",
      "Epoch 4/5\n",
      "338/338 [==============================] - 517s 2s/step - loss: 0.3473 - tp: 18639.5723 - fp: 2165.1504 - tn: 41352.6195 - fn: 3119.3127 - accuracy: 0.9174 - precision: 0.8939 - recall: 0.8533 - auc: 0.9675 - val_loss: 0.7314 - val_tp: 8018.0000 - val_fp: 2321.0000 - val_tn: 19249.0000 - val_fn: 2767.0000 - val_accuracy: 0.8427 - val_precision: 0.7755 - val_recall: 0.7434 - val_auc: 0.8976\n",
      "Epoch 5/5\n",
      "338/338 [==============================] - 516s 2s/step - loss: 0.2445 - tp: 19546.3835 - fp: 1573.8820 - tn: 41943.8879 - fn: 2212.5015 - accuracy: 0.9437 - precision: 0.9274 - recall: 0.9016 - auc: 0.9832 - val_loss: 0.8308 - val_tp: 7948.0000 - val_fp: 2435.0000 - val_tn: 19135.0000 - val_fn: 2837.0000 - val_accuracy: 0.8371 - val_precision: 0.7655 - val_recall: 0.7369 - val_auc: 0.8923\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "      optimizer='adam',\n",
    "      metrics=METRICS)\n",
    "history = model.fit(X_train,y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdee81d4c10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.predict(X_test)\n",
    "#print ('logits:',logits[:10])\n",
    "probi = logits[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13631,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.argmax(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df1 = pd.DataFrame(logits.argmax(axis=1), columns=['lstm_pred'])\n",
    "lstm_df2 = pd.DataFrame(logits, columns=['lstm_0','lstm_1','lstm_2'])\n",
    "tradition_mp = pd.concat([df_test, lstm_df1, lstm_df2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradition_mp.to_csv('./Dataset/lstm_mp.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_evalute(frame, columns):\n",
    "    \n",
    "    ## split testing set\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.9, random_state=11)\n",
    "    accuracy, precision, recall, f1, auc = [], [], [], [], []\n",
    "    ## 90% testing \n",
    "    for dropped_index,valid_index in sss.split(frame, frame['Tag']):\n",
    "        y_valid = frame['Tag'][valid_index]\n",
    "        y_pred = frame[columns[0]][valid_index]\n",
    "        y_pred_prob = frame[columns[1:]].iloc[valid_index].to_numpy()\n",
    "\n",
    "        accuracy.append(mtc.accuracy_score(y_valid, y_pred))\n",
    "        precision.append(mtc.precision_score(y_valid, y_pred, average='weighted'))\n",
    "        recall.append(mtc.recall_score(y_valid, y_pred, average='weighted'))\n",
    "        f1.append(mtc.f1_score(y_valid, y_pred, average='weighted'))\n",
    "        auc.append(mtc.roc_auc_score(y_valid, y_pred_prob,multi_class='ovo'))\n",
    "        \n",
    "    return accuracy, precision, recall, f1, auc\n",
    "\n",
    "\n",
    "def label_posts(character):\n",
    "    if character == 'I am their child':\n",
    "        return 0\n",
    "    elif character == 'I am their partner or spouse':\n",
    "        return 1\n",
    "    elif character == 'Other':\n",
    "        return 2\n",
    "    else:\n",
    "        print ('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383cf138bf2a4ceb91d9db3c1255017a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Pandas Apply', max=13631.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tradition_mp['Tag'] = tradition_mp.Label.swifter.apply(lambda x: label_posts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a26eb546bc46268149f34b5f7f183d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dask Apply', max=32.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "author_list = tradition_mp[['user_id','Tag']].drop_duplicates().reset_index(drop=True)\n",
    "author_list = author_list.join(tradition_mp.groupby('user_id').apply(lambda x: np.mean(x[['lstm_0','lstm_1','lstm_2']])), how='left',on='user_id')\n",
    "#author_list = author_list.join(tradition_mp.groupby('user_id').apply(lambda x: np.mean(x[['RF_0','RF_1','RF_2']])), how='left',on='user_id')\n",
    "#author_list = author_list.join(tradition_mp.groupby('user_id').apply(lambda x: np.mean(x[['KNN_0','KNN_1','KNN_2']])), how='left',on='user_id')\n",
    "\n",
    "author_list['lstm_pred'] = author_list.swifter.apply(lambda x: x[['lstm_0','lstm_1','lstm_2']].argmax(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_evalute_seperate(frame, columns):\n",
    "    \n",
    "    ## split testing set\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.9, random_state=11)\n",
    "    accuracy, precision, recall, f1, auc = [], [], [], [], []\n",
    "    pre_all, rec_all, f1_all = [],[],[]\n",
    "    ## 90% testing \n",
    "    for dropped_index,valid_index in sss.split(frame, frame['Tag']):\n",
    "        y_valid = frame['Tag'][valid_index]\n",
    "        y_pred = frame[columns[0]][valid_index]\n",
    "        y_pred_prob = frame[columns[1:]].iloc[valid_index].to_numpy()\n",
    "\n",
    "        accuracy.append(mtc.accuracy_score(y_valid, y_pred))\n",
    "        precision.append(mtc.precision_score(y_valid, y_pred, average='weighted'))\n",
    "        recall.append(mtc.recall_score(y_valid, y_pred, average='weighted'))\n",
    "        f1.append(mtc.f1_score(y_valid, y_pred, average='weighted'))\n",
    "        auc.append(mtc.roc_auc_score(y_valid, y_pred_prob,multi_class='ovo'))\n",
    "        \n",
    "        pre_all.append(mtc.precision_score(y_valid, y_pred, average=None))\n",
    "        rec_all.append(mtc.recall_score(y_valid, y_pred, average=None))\n",
    "        f1_all.append(mtc.f1_score(y_valid, y_pred, average=None))\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc, pre_all, rec_all, f1_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.738 (0.001)\n",
      "Precision:\t0.725 (0.001)\n",
      "Recall:\t\t0.738 (0.001)\n",
      "F1:\t\t0.728 (0.001)\n",
      "AUC:\t\t0.821 (0.001)\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, auc = performance_evalute(tradition_mp,['lstm_pred','lstm_0','lstm_1','lstm_2'])\n",
    "print ('Accuracy:\\t%.3f (%.3f)'%(np.mean(accuracy), np.std(accuracy)))\n",
    "print ('Precision:\\t%.3f (%.3f)'%(np.mean(precision),np.std(precision)))\n",
    "print ('Recall:\\t\\t%.3f (%.3f)'%(np.mean(recall),np.std(recall)))\n",
    "print('F1:\\t\\t%.3f (%.3f)'%(np.mean(f1),np.std(f1)))\n",
    "print('AUC:\\t\\t%.3f (%.3f)'%(np.mean(auc),np.std(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.811 (0.002)\n",
      "Precision:\t0.800 (0.002)\n",
      "Recall:\t\t0.811 (0.002)\n",
      "F1:\t\t0.795 (0.002)\n",
      "AUC:\t\t0.871 (0.002)\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, auc = performance_evalute(author_list,['lstm_pred','lstm_0','lstm_1','lstm_2'])\n",
    "print ('Accuracy:\\t%.3f (%.3f)'%(np.mean(accuracy), np.std(accuracy)))\n",
    "print ('Precision:\\t%.3f (%.3f)'%(np.mean(precision),np.std(precision)))\n",
    "print ('Recall:\\t\\t%.3f (%.3f)'%(np.mean(recall),np.std(recall)))\n",
    "print('F1:\\t\\t%.3f (%.3f)'%(np.mean(f1),np.std(f1)))\n",
    "print('AUC:\\t\\t%.3f (%.3f)'%(np.mean(auc),np.std(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.77324855 0.76052778 0.4664147 ] [0.00137166 0.00259033 0.00338791]\n",
      "Recall: [0.86780909 0.67468023 0.3409734 ] [0.00142498 0.00271285 0.00219549]\n",
      "f1: [0.817804   0.71503528 0.39394579] [0.00126341 0.00250939 0.00240061]\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, auc, pre_all, recal_all, f1_all = performance_evalute_seperate(tradition_mp,['lstm_pred','lstm_0','lstm_1','lstm_2'])\n",
    "print('Precision:',np.stack(pre_all).mean(axis=0),np.stack(pre_all).std(axis=0))\n",
    "print('Recall:',np.stack(recal_all).mean(axis=0),np.stack(recal_all).std(axis=0))\n",
    "print('f1:',np.stack(f1_all).mean(axis=0),np.stack(f1_all).std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.82076829 0.83939715 0.67798855] [0.00134482 0.00366352 0.00882009]\n",
      "Recall: [0.94038756 0.81563307 0.36948276] [0.00172855 0.00258721 0.00481834]\n",
      "f1: [0.87651493 0.82733943 0.47829164] [0.00131222 0.00239613 0.0057238 ]\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, auc, pre_all, recal_all, f1_all = performance_evalute_seperate(author_list,['lstm_pred','lstm_0','lstm_1','lstm_2'])\n",
    "print('Precision:',np.stack(pre_all).mean(axis=0),np.stack(pre_all).std(axis=0))\n",
    "print('Recall:',np.stack(recal_all).mean(axis=0),np.stack(recal_all).std(axis=0))\n",
    "print('f1:',np.stack(f1_all).mean(axis=0),np.stack(f1_all).std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
