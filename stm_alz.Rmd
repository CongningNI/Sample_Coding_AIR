---
title: "STM_alzconnect"
author: "congningNi"
output: html_document
---

## Import libraries

```{r}
install.packages("stm") # package installations are only needed the first time you use it
#install.packages("tm") 
install.packages("furrr")    # alternative installation of the %>%
install.packages("dplyr")
install.packages("tidyverse")
```

## Load libraries

```{r}
library(stm)
library(tm)
library(furrr)
#library(dplyr) 
library(ggplot2)
library(tidyr)
library(tidyverse)
## Load package
library(igraph)
```

```{r}
packageVersion("future")
future.seed=TRUE
```

## Set dataset

```{r import}
set.seed(2022)
rel_df <- read.csv("../Dataset/stm_relationships_oct5.csv")
#rel_df <- rel_df[sample(1:nrow(data), 1000), ]
processed <- textProcessor(rel_df$clean_content, metadata = rel_df)
rel_tmp1 <- rel_df[-processed$docs.removed,]
processed <- textProcessor(rel_tmp1$clean_content, metadata = rel_tmp1)
```
```{r}
out <- prepDocuments(processed$documents, processed$vocab, processed$meta,lower.thresh=10)
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```
```{r}
length(rel_tmp1$clean_content)
length(processed$documents)
length(out$documents)
length(out$docs.removed)
```

## Searching K

```{r}
set.seed(1127)
plan(multisession)
```

```{r}
options(future.globals.maxSize = 600*1024^2)
many_models <- tibble(K = c(10,15,20,25,30)) %>%
  mutate(topic_model = future_map(K, ~stm(documents = out$documents, vocab = out$vocab,K = ., 
                                          prevalence =~Relationship + crosscomments + s(solidmonths) + s(usermonths) + s(relemonths) + s(replies), 
                                          max.em.its = 20, data = out$meta, init.type = "LDA",verbose = FALSE)))
```

```{r}
heldout <- make.heldout(documents = out$documents, vocab = out$vocab, seed=1127)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, out$documents),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, out$documents),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))
```

```{r k result}
k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Diagnostics by number of topics")
```

```{r}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(5,10,15,20,25,30)) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(aes(shape=K), alpha = 0.8) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = '')
```

## STM models

```{r}
stm25_alz<- stm(documents = out$documents, 
            vocab = out$vocab, 
            K = 25, 
            data = out$meta, 
            #content =~ subreddit, 
            prevalence = ~Relationship + crosscomments + s(solidmonths) + s(usermonths) + s(relemonths) + s(replies), 
            max.em.its = 150,
            init.type = "LDA",
            seed = 1127
            )
```

## STM evaluation

```{r}
plot(stm25_alz, type = "summary", xlim = c(0, 0.3),n=8,text.cex = 0.8)
labelTopics(stm25_alz,c(1:25), n=8)
```

```{r}
alz_cormat <- topicCorr(stm25_alz)
plot(alz_cormat,vertex.color='lightblue')
```

```{r}
prep_alz <- estimateEffect(formula=1:25 ~Relationship + crosscomments + s(solidmonths) + s(usermonths) + s(relemonths) + s(replies),
                                    stm25_alz,  meta = out$meta, uncertainty = "Global")
summary(prep_alz, topics = c(1:25))

```

```{r}
topicNames<-c("Topic 1","Topic 2","Topic 3","Topic 4","Topic 5","Topic 6",
          "Topic 7","Topic 8","Topic 9","Topic 10","Topic 11","Topic 12",
          "Topic 13","Topic 14","Topic 15","Topic 16","Topic 17",
          "Topic 18","Topic 19","Topic 20","Topic 21","Topic 22",
          "Topic 23","Topic 24","Topic 25")
plot(prep_alz, covariate = "Relationship", topics = c(1:25),
        model = stm25_alz, method = "difference",
        cov.value1 = 'Partner', cov.value2 = 'Child',
        xlab = "Child ... Partner",
        main = "Effect",
        labeltype=c("custom"),custom.labels = topicNames,
        xlim = c(-.06, .06), width=30)

plot(prep_alz, covariate = "crosscomments", topics = c(1:25),
        model = stm25_alz, method = "difference",
        cov.value1 = "Cross", cov.value2 = "No Cross",
        xlab = "No Cross ... Cross",
        main = "Effect",
        labeltype=c("custom"),custom.labels = topicNames,
        xlim = c(-.06, .06), width=30)

```

```{r}
plot(prep_alz, "solidmonths", method = "continuous", topics = 3,model = stm25_alz, printlegend = TRUE, xaxt = "n", xlab = "Time")
monthseq <- seq(from = as.Date("2011-10-01"),to = as.Date("2022-08-01"), by = "month")
monthnames <- format(as.Date(monthseq), "%Y-%m")
n = length(monthnames)
axis(1,at = seq(1,n,2), labels = monthnames[seq(1,n,2)])
#axis(1,at = as.numeric(monthseq) - min(as.numeric(monthseq)),labels = monthnames)
```

```{r}
p <- plot(prep_alz, "relemonths", method = "continuous", topics = 3,model = stm25_alz)
p + xlab("Active days") + ylab("Probability of correct labels") +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  scale_color_manual(values=c("#999999", "#E69F00")) +
    theme(axis.text = element_text(size=15), axis.title = element_text(size=15), legend.text = element_text(size=15), legend.title = element_text(size=15)) 

```

#export topic words as table

```{r topics}
topics<-labelTopics(stm25_alz, n = 15)
#topics$prob
topics$topicnums
```

```{r}

df <- data.frame(matrix(ncol = 3, nrow = 0))
col_headings <- c('Topic','Metric','Topic Words')
names(df) <- col_headings
t <- topics
for (i in 1:25){
  ind = paste("Topic #", i)
  start = (i-1)*4+1
  df[start,1] <- ind
  df[start,2] = "prob"
  df[start,3] = toString(t$prob[i,])
  
  df[start+1,1] <- ind
  df[start+1,2] = "frex"
  df[start+1,3] = toString(t$frex[i,])
  
  df[start+2,1] <- ind
  df[start+2,2] = "lift"
  df[start+2,3] = toString(t$lift[i,])
  
  df[start+3,1] <- ind
  df[start+3,2] = "score"
  df[start+3,3] = toString(t$score[i,])
}
write.csv(df, file = '../Dataset/stm25_Sep26.csv', row.names = FALSE, na = '')
```

```{r}
#alz_cormat$poscor
#summary(stm25_alz$theta)
#as.data.frame(colSums(stm25_alz$theta/nrow(stm25_alz$theta)))
alz_corr <- colSums(stm25_alz$theta/nrow(stm25_alz$theta))
alz_corr
```

```{r}

df <- data.frame(matrix(ncol = 4, nrow = 0))
col_headings <- c('Topic','Metric','Topic Words','Corrlation')
names(df) <- col_headings
t <- topics
for (i in 1:25){
  ind = paste("Topic #", i)
  start = (i-1)*4+1
  df[start,1] <- ind
  df[start,2] = "prob"
  df[start,3] = toString(t$prob[i,])
  df[start,4] = alz_corr[i]
  
  df[start+1,1] <- ind
  df[start+1,2] = "frex"
  df[start+1,3] = toString(t$frex[i,])
  df[start+1,4] = alz_corr[i]
  
  df[start+2,1] <- ind
  df[start+2,2] = "lift"
  df[start+2,3] = toString(t$lift[i,])
  df[start+2,4] = alz_corr[i]
  
  df[start+3,1] <- ind
  df[start+3,2] = "score"
  df[start+3,3] = toString(t$score[i,])
  df[start+3,4] = alz_corr[i]
}
write.csv(df, file = '../Dataset/stm25_words_corr_Sep26.csv', row.names = FALSE, na = '')
```

```{r store prevalence}
select_topics <- topics$prob
for (i in 1:25){
  png(paste("./topics_trends/Absolute Timeline/","abst_Topic#",i,".png",sep=""),width = 1000, height = 500, units = "px", res=120)
par(mfrow=c(1,1))
par(mar=c(5.1,4.1,4.1,2.1))
  plot(prep_alz, "solidmonths", method = "continuous", topics = i,model = stm25_alz, printlegend = FALSE, xaxt = "n", xlab = "Absolute Months",   main=paste("Topic",i,":",select_topics[i,1],select_topics[i,2],select_topics[i,3], sep=" "),
       text.cex=5) 
  monthseq <- seq(from = as.Date("2011-10-01"),to = as.Date("2022-08-01"), by = "month")
  monthnames <- format(as.Date(monthseq), "%Y-%m")
  n = length(monthnames)
  axis(1,at = seq(1,n,2), labels = monthnames[seq(1,n,2)])
dev.off()
}
```

plot(prep_alz, "solidmonths", method = "continuous", topics = 3,model = stm25_alz, printlegend = TRUE, xaxt = "n", xlab = "Time") monthseq \<- seq(from = as.Date("2011-10-01"),to = as.Date("2022-08-01"), by = "month") monthnames \<- format(as.Date(monthseq), "%Y-%m") n = length(monthnames) axis(1,at = seq(1,n,2), labels = monthnames[seq(1,n,2)])

```{r store prevalence}
select_topics <- topics$prob
for (i in 1:25){
  png(paste("./topics_trends/Relative Timeline/","relat_Topic#",i,".png",sep=""),width = 1000, height = 500, units = "px", res=120)
par(mfrow=c(1,1))
par(mar=c(5.1,4.1,4.1,2.1))
  plot(prep_alz, "relemonths", method = "continuous", topics = i,model = stm25_alz, xlab = "Relative Month(s)",   printlegend = FALSE, main=paste("Topic",i,":",select_topics[i,1],select_topics[i,2],select_topics[i,3], sep=" "),
       text.cex=5) 
dev.off()
}
```

```{r store prevalence}
select_topics <- topics$prob
for (i in 1:25){
  png(paste("./topics_trends/User Registed Timeline/","usert_Topic#",i,".png",sep=""),width = 1000, height = 500, units = "px", res=120)
par(mfrow=c(1,1))
par(mar=c(5.1,4.1,4.1,2.1))
  plot(prep_alz, "usermonths", method = "continuous", topics = i,model = stm25_alz, printlegend = FALSE, xaxt = "n", xlab = "User Register Months",   main=paste("Topic",i,":",select_topics[i,1],select_topics[i,2],select_topics[i,3], sep=" "),
       text.cex=5) 
  monthseq <- seq(from = as.Date("2011-10-01"),to = as.Date("2022-08-01"), by = "month")
  monthnames <- format(as.Date(monthseq), "%Y-%m")
  n = length(monthnames)
  axis(1,at = seq(1,n,2), labels = monthnames[seq(1,n,2)])
dev.off()
}
```

```{r store prevalence}
select_topics <- topics$prob
for (i in 1:25){
  png(paste("./topics_trends/Replies/","reply_Topic#",i,".png",sep=""),width = 1000, height = 500, units = "px", res=120)
par(mfrow=c(1,1))
par(mar=c(5.1,4.1,4.1,2.1))
  plot(prep_alz, "replies", method = "continuous", topics = i,model = stm25_alz, xlab = "Count of Replies",   printlegend = FALSE, main=paste("Topic",i,":",select_topics[i,1],select_topics[i,2],select_topics[i,3], sep=" "),
       text.cex=5) 
dev.off()
}
```

```{r}
##16, 21, 25
z<-rel_tmp1[-out$docs.removed,]

thought1 <-  findThoughts(stm25_alz, texts = z$clean_content, topics=c(16,21,25), n=1000)
post_100 <- z[unlist(thought1$index),]
write.csv(post_100, '100posts_dementia.csv')
#plot(thought1, width=130, text.cex = 0.6)
```

```{r}
##16, 21, 25

thought2 <-  findThoughts(stm25_alz, texts = z$clean_content, topics=c(1,2,3,6,7,8,9,10,11,12,13,14,15,16,17,18,20,21,24,25), n=1000)
post_100 <- z[unlist(thought2$index),]
write.csv(post_100, '500posts_all.csv')
#plot(thought1, width=130, text.cex = 0.6)
```


```{r}
z[c(26597),]
z[unlist(thought1$index),]

unlist(thought1$index)[1][1]
thought1$docs
#typeof(thought1$index)
#typeof(c(1,3))
```
```{r}
library(tidyverse)
prep_alz$data
#summary(prep_alz, topics = c(1:15))
#summary(prep_alz)
```

```{r refit}
testing_set <- read_csv('./testing_set.csv')

```
```{r refit}
testing_docs <- textProcessor(documents = testing_set$Content, metadata = testing_set)

```
```{r refit}
new_docs <- alignCorpus(new=testing_docs, old.vocab = stm25_alz$vocab)
newdocs_fit <- fitNewDocuments(model=stm25_alz, documents = new_docs$documents, newData = new_docs$meta, origData = out$meta)
```
```{r refit}
library(MASS)
class(newdocs_fit$theta)
write.matrix(newdocs_fit$theta, file='newfit_matrix.csv')
```


